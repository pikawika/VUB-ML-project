% done
\part{Gradient Boosting}
\label{part:gradien_boost}

%------------------------------------

\section{About this part}
\label{section:gb_about_part}

All previous models where standalone classifiers, to make things more interesting an ensemble is tried next.
An ensemble consists of cleverly combining multiple \textit{weaker} classifiers to form a final \textit{strong} classifier.
Gradient Boosting and random forest are common ensembles.
A quick experiment showed that default Gradient Boosting performed better than an optimised random forest (MCLL of 1.68 vs 1.74).
Thus, it is chosen to only discuss Gradient Boosting further.
The Notebook corresponding with this part is \texttt{gradient\_boosting.ipynb}.

%------------------------------------

\section{Scoring and methodology for fine-tuning}
\label{section:gb_methodology}

Fine-tuning and evaluation of the model is done in an equal manner as the previous models.
This model doesn't have a parameter like class weight which specifies to work in a balanced manner.
Due to the many parameters that need fine-tuning and limited computing power, the use of \texttt{GridSearchCV} was split over 2 independent searches.
Found optima were retested to validate they don't influence previous determined parameters.
The tried parameters using \texttt{GridSearchCV} are listed here:
\begin{itemize}
    \item Experiment 1:
    \begin{itemize}
        \item Tested loss: deviance and exponential.
        \item Tested learning rates: 0.001, 0.01 , 0.1 and 0.5.
        \item Tested amount of estimators: 50, 100, 250, 500, 600, 750 and 1000.
    \end{itemize}
    \item Experiment 2:
    \begin{itemize}
        \item Tested subsample: 0.5, 0.75, 1.0 and 1.5.
        \item Tested minimum sample split: 2, 3 and 5.
        \item Tested max depth: 3, 5, 7 and 10.
    \end{itemize}
\end{itemize}

%------------------------------------

\section{Interesting but not impressive}
\label{section:gb_optimal}

Whilst the Gradient Boosting approach is interesting, it didn't score well and there are signs of overfitting with an MCLL score of ± 0.21 on the training data using some settings.
The latter could perhaps be resolved but since the model performed worse than the linear baseline model throughout testing, it was not explored further.
Only SIFT with 100 clusters was considered.
A MCLL score of ± 1.68 on the validation set, a SCV MCLL score of ± 1.62 and a Kaggle score of 1.70494 were achieved with the following parameters:
\begin{itemize}
    \item Loss and learning rate: deviance and 0.01.
    \item Amount of estimators and subsample: 750 and 0.75.
    \item Minimum sample split and max depth: 5.
\end{itemize}